{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import nltk.classify\n",
    "from tweepy import OAuthHandler\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tweepy.streaming import StreamListener\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTagsAndLinks(tweet):\n",
    "    cleaned_tweet=re.sub(\"@[A-Za-z0-9]+|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\"\",tweet)\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(tweet):\n",
    "    cleaned_tweet=removeTagsAndLinks(tweet)\n",
    "    sentences=sent_tokenize(cleaned_tweet)\n",
    "    word=RegexpTokenizer(r'\\w+')\n",
    "    words=[word.tokenize(sentence) for sentence in sentences]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceTwoOrMore(s):\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "\n",
    "def processTweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)    \n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    stopWords = []\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "\n",
    "    fp = open(r'stopwordsID.txt', 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords\n",
    "\n",
    "\n",
    "def getFeatureVector(tweet, stopWords):\n",
    "    featureVector = []  \n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        w = replaceTwoOrMore(w) \n",
    "        w = w.strip('\\'\"?,.')\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*[a-zA-Z]+[a-zA-Z0-9]*$\", w)\n",
    "        if(w in stopWords or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector    \n",
    "\n",
    "def removeStopwords(tweet):\n",
    "    stop_words=set(stopwords.words('indonesian'))\n",
    "    filtered_tweet=[word for dummytweet in tweet for word in  dummytweet  if word not in stop_words]\n",
    "    return filtered_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(tweet):\n",
    "    converted_tweet=''\n",
    "    for word in tweet:\n",
    "        converted_tweet+=word+\" \"\n",
    "    return converted_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authorize():\n",
    "    cunsomer_key = 'd3GCOcnCQVrHqYdbkA6p5B0sb'\n",
    "    consumer_secret = '3XqfMNq4Aj6rnuUjjV6ZCiQdt4SPeWG6N5l5CcrXf1C540vpnV'\n",
    "    access_token = '772637934390345728-ZtWEKm6pab38OFDtXqlRgjcEvmdRZ9v'\n",
    "    access_token_secret = 'MHx06gmb5SC4EeXIkvJFKMwyxZD3PMDBOGoGyH6j6fC6A'\n",
    "    auth = tweepy.OAuthHandler(cunsomer_key , consumer_secret)\n",
    "    auth.set_access_token(access_token,access_token_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(query,limit=100000,language='in'):\n",
    "    list_tweet = []\n",
    "    list_kelas = []\n",
    "    summary={'Positive':0,'Negative':0,'Neutral':0}\n",
    "    api=authorize()\n",
    "    sid=SentimentIntensityAnalyzer()\n",
    "    \n",
    "    tweets=tweepy.Cursor(api.search,q=query,lang=language).items(limit)\n",
    "        \n",
    "    for tweet in tweets:\n",
    "        tweet_text= tweet.text\n",
    "        tweet=tokenise(tweet_text)\n",
    "        tweet=removeStopwords(tweet)\n",
    "        tweet=convert(tweet)\n",
    "        score=sid.polarity_scores(tweet)\n",
    "        sentiment = ''\n",
    "        if score['compound']==0:\n",
    "            summary['Neutral']+=1\n",
    "            sentiment = 'Neutral'\n",
    "        elif score['compound']>0:\n",
    "            summary['Positive']+=1\n",
    "            sentiment = 'Positive'\n",
    "        else:\n",
    "            summary['Negative']+=1\n",
    "            sentiment = 'Negative'\n",
    "        list_tweet.append(tweet)\n",
    "        list_kelas.append(sentiment)\n",
    "    for keys in summary.keys():\n",
    "        print(\"No of %s tweets are %d\"%(keys,summary[keys]))\n",
    "    df = pd.DataFrame({'tweet': list_tweet,'kelas': list_kelas})\n",
    "    df.to_csv('data_crawling.csv', sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    query=input('Enter the key to be searched:')\n",
    "    no=int(input('Enter the number of tweets to be analyzed:'))\n",
    "    try:\n",
    "        language=input('Enter the language:')\n",
    "    except:\n",
    "        language='in'\n",
    "    print('====================================================')\n",
    "    analyze(query,no,language)\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23899\n"
     ]
    }
   ],
   "source": [
    "list_data = []\n",
    "reader = csv.reader(open(\"datatrain.csv\"))\n",
    "stopWords = getStopWordList('stopwordsID.txt')\n",
    "total = 0\n",
    "for index,raw in enumerate(reader):\n",
    "    if index == 0:\n",
    "        pass\n",
    "    else:\n",
    "        sentiment = raw[1]\n",
    "        tweet = raw[2]\n",
    "        tweet = processTweet(tweet)\n",
    "        tweet = removeTagsAndLinks(tweet)\n",
    "        tweet = getFeatureVector(tweet,stopWords)\n",
    "        tweet = ' '.join(tweet)\n",
    "        mydata = (tweet,sentiment)\n",
    "        list_data.append(mydata)\n",
    "        total += 1\n",
    "\n",
    "print(total)\n",
    "json_dict = {}\n",
    "all_words = set(word.lower() for passage in list_data for word in word_tokenize(passage[0]))\n",
    "json_dict['corpus'] = [word for word in all_words] \n",
    "\n",
    "with open('corpus.json', 'w') as outfile:\n",
    "    json.dump(json_dict, outfile)\n",
    "\n",
    "t = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in list_data]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "\n",
    "f = open('sentiment.pickle', 'wb')\n",
    "pickle.dump(classifier, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.400000000000006\n"
     ]
    }
   ],
   "source": [
    "stopWords = getStopWordList('stopwordsID.txt')\n",
    "f = open('sentiment.pickle', 'rb')\n",
    "classifier = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "d = open('corpus.json')\n",
    "all_words = json.load(d)\n",
    "d.close\n",
    "all_words = all_words['corpus']\n",
    "\n",
    "list_tweet =  []\n",
    "list_kelas = []\n",
    "list_data = []\n",
    "\n",
    "test_set = csv.reader(open('new_sefty.csv', 'r'), delimiter=',')\n",
    "data_list = []\n",
    "      \n",
    "\n",
    "for index,data in enumerate(test_set):\n",
    "    if index == 0:\n",
    "        pass\n",
    "    else:\n",
    "        sentiment = data[1]\n",
    "        tweet = data[2]\n",
    "        tweet = processTweet(tweet)\n",
    "        tweet = removeTagsAndLinks(tweet)\n",
    "        tweet = getFeatureVector(tweet,stopWords)\n",
    "        tweet = ' '.join(tweet)\n",
    "        test_sent_features = {word.lower(): (word in word_tokenize(tweet.lower())) for word in all_words}\n",
    "        prediksi = classifier.classify(test_sent_features)\n",
    "        list_tweet.append(data[2])\n",
    "        list_kelas.append(prediksi)\n",
    "        mydata = (tweet,sentiment)\n",
    "        data_list.append(mydata)\n",
    "\n",
    "df = pd.DataFrame({'tweet': list_tweet,'kelas': list_kelas})\n",
    "df.to_csv('hasiltest.csv', sep=';', encoding='utf-8', index=False)\n",
    "        \n",
    "test_set = [({word.lower() : (word.lower() in word_tokenize(x[0])) for word in all_words}, x[1]) for x in data_list]\n",
    "accuracy = nltk.classify.util.accuracy(classifier, test_set)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               menyentuh = True           negati : Positi =   1863.0 : 1.0\n",
      "              tersandung = True           negati : Positi =   1863.0 : 1.0\n",
      "                prihatin = True           negati : Positi =   1117.8 : 1.0\n",
      "                   susah = True           negati : Positi =   1117.8 : 1.0\n",
      "                   kepri = True           negati : Positi =    621.0 : 1.0\n",
      "                kadernya = True           negati : Positi =    508.1 : 1.0\n",
      "                    riau = True           negati : Negati =    490.1 : 1.0\n",
      "                 positif = True           negati : Positi =    483.0 : 1.0\n",
      "                     top = True           negati : Positi =    372.6 : 1.0\n",
      "                  kontes = True           Positi : Positi =    285.0 : 1.0\n",
      "                   domba = True           Positi : Negati =    283.6 : 1.0\n",
      "                     bnn = True           negati : Negati =    272.3 : 1.0\n",
      "                 edarkan = True           negati : Negati =    272.3 : 1.0\n",
      "                  attack = True           Negati : Positi =    271.9 : 1.0\n",
      "                     war = True           Netral : Positi =    254.0 : 1.0\n",
      "                   barat = True           negati : Positi =    253.2 : 1.0\n",
      "                   kaget = True           Negati : Positi =    245.4 : 1.0\n",
      "             kebangkitan = True           Positi : Positi =    223.2 : 1.0\n",
      "                 kambing = True           Netral : Positi =    218.6 : 1.0\n",
      "                    kill = True           Negati : Positi =    205.6 : 1.0\n",
      "               berhadiah = True           Positi : Positi =    188.9 : 1.0\n",
      "                  matang = True           Positi : Positi =    188.9 : 1.0\n",
      "               bersihkan = True           Positi : Negati =    168.1 : 1.0\n",
      "                    tamu = True           Negati : Positi =    165.8 : 1.0\n",
      "                    bayi = True           negati : Positi =    159.7 : 1.0\n",
      "                shanghai = True           Netral : Positi =    159.5 : 1.0\n",
      "                    navy = True           Positi : Negati =    158.1 : 1.0\n",
      "                     pln = True           negati : Negati =    144.2 : 1.0\n",
      "                 menpora = True           Positi : Negati =    143.0 : 1.0\n",
      "                      in = True           Negati : Negati =    136.7 : 1.0\n",
      "                    news = True           negati : Netral =    135.2 : 1.0\n",
      "                olahraga = True           Positi : Positi =    133.9 : 1.0\n",
      "                 militan = True           Negati : Positi =    126.0 : 1.0\n",
      "                penamaan = True           Netral : Positi =    124.1 : 1.0\n",
      "                     for = True           Netral : Positi =    124.1 : 1.0\n",
      "                   dhaka = True           Negati : Positi =    122.2 : 1.0\n",
      "                  simbol = True           Positi : Positi =    120.2 : 1.0\n",
      "                     bus = True           negati : Positi =    114.1 : 1.0\n",
      "                sutiyoso = True           Negati : Positi =    112.8 : 1.0\n",
      "                  menpar = True           Netral : Negati =    111.4 : 1.0\n",
      "                    jual = True           negati : Positi =    108.5 : 1.0\n",
      "                 pegawai = True           negati : Positi =    105.5 : 1.0\n",
      "                 narkoba = True           negati : Netral =    105.1 : 1.0\n",
      "               pontianak = True           negati : Netral =    105.1 : 1.0\n",
      "                   istri = True           negati : Netral =    105.1 : 1.0\n",
      "                  pantai = True           Positi : Positi =    104.6 : 1.0\n",
      "                terserah = True           Netral : Positi =    100.4 : 1.0\n",
      "                    five = True           Negati : Positi =     99.5 : 1.0\n",
      "                  sayyaf = True           Negati : Positi =     99.5 : 1.0\n",
      "                personel = True           Positi : Negati =     97.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "feature = classifier.show_most_informative_features(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
