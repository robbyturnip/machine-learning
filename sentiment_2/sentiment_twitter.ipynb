{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import nltk.classify\n",
    "from tweepy import OAuthHandler\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tweepy.streaming import StreamListener\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTagsAndLinks(tweet):\n",
    "    cleaned_tweet=re.sub(\"@[A-Za-z0-9]+|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\"\",tweet)\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(tweet):\n",
    "    cleaned_tweet=removeTagsAndLinks(tweet)\n",
    "    sentences=sent_tokenize(cleaned_tweet)\n",
    "    word=RegexpTokenizer(r'\\w+')\n",
    "    words=[word.tokenize(sentence) for sentence in sentences]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceTwoOrMore(s):\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "\n",
    "def processTweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    tweet = re.sub('@[^\\s]+','AT_USER',tweet)    \n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def getStopWordList(stopWordListFileName):\n",
    "    stopWords = []\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "\n",
    "    fp = open(r'stopwordsID.txt', 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords\n",
    "\n",
    "\n",
    "def getFeatureVector(tweet, stopWords):\n",
    "    featureVector = []  \n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        w = replaceTwoOrMore(w) \n",
    "        w = w.strip('\\'\"?,.')\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*[a-zA-Z]+[a-zA-Z0-9]*$\", w)\n",
    "        if(w in stopWords or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector    \n",
    "\n",
    "def removeStopwords(tweet):\n",
    "    stop_words=set(stopwords.words('indonesian'))\n",
    "    filtered_tweet=[word for dummytweet in tweet for word in  dummytweet  if word not in stop_words]\n",
    "    return filtered_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(tweet):\n",
    "    converted_tweet=''\n",
    "    for word in tweet:\n",
    "        converted_tweet+=word+\" \"\n",
    "    return converted_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authorize():\n",
    "    cunsomer_key = 'd3GCOcnCQVrHqYdbkA6p5B0sb'\n",
    "    consumer_secret = '3XqfMNq4Aj6rnuUjjV6ZCiQdt4SPeWG6N5l5CcrXf1C540vpnV'\n",
    "    access_token = '772637934390345728-ZtWEKm6pab38OFDtXqlRgjcEvmdRZ9v'\n",
    "    access_token_secret = 'MHx06gmb5SC4EeXIkvJFKMwyxZD3PMDBOGoGyH6j6fC6A'\n",
    "    auth = tweepy.OAuthHandler(cunsomer_key , consumer_secret)\n",
    "    auth.set_access_token(access_token,access_token_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(query,limit=100000,language='in'):\n",
    "    list_tweet = []\n",
    "    list_kelas = []\n",
    "    summary={'Positive':0,'Negative':0,'Neutral':0}\n",
    "    api=authorize()\n",
    "    sid=SentimentIntensityAnalyzer()\n",
    "    \n",
    "    tweets=tweepy.Cursor(api.search,q=query,lang=language).items(limit)\n",
    "        \n",
    "    for tweet in tweets:\n",
    "        tweet_text= tweet.text\n",
    "        tweet=tokenise(tweet_text)\n",
    "        tweet=removeStopwords(tweet)\n",
    "        tweet=convert(tweet)\n",
    "        score=sid.polarity_scores(tweet)\n",
    "        sentiment = ''\n",
    "        if score['compound']==0:\n",
    "            summary['Neutral']+=1\n",
    "            sentiment = 'Neutral'\n",
    "        elif score['compound']>0:\n",
    "            summary['Positive']+=1\n",
    "            sentiment = 'Positive'\n",
    "        else:\n",
    "            summary['Negative']+=1\n",
    "            sentiment = 'Negative'\n",
    "        list_tweet.append(tweet)\n",
    "        list_kelas.append(sentiment)\n",
    "    for keys in summary.keys():\n",
    "        print(\"No of %s tweets are %d\"%(keys,summary[keys]))\n",
    "    df = pd.DataFrame({'tweet': list_tweet,'kelas': list_kelas})\n",
    "    df.to_csv('dataset.csv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the key to be searched:dpr\n",
      "Enter the number of tweets to be analyzed:100\n",
      "Enter the language:in\n",
      "====================================================\n",
      "No of Positive tweets are 6\n",
      "No of Negative tweets are 0\n",
      "No of Neutral tweets are 94\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    query=input('Enter the key to be searched:')\n",
    "    no=int(input('Enter the number of tweets to be analyzed:'))\n",
    "    try:\n",
    "        language=input('Enter the language:')\n",
    "    except:\n",
    "        language='in'\n",
    "    print('====================================================')\n",
    "    analyze(query,no,language)\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_test():\n",
    "    negative_tweets = (df_test[df_test.sentiment==0].text).tolist()\n",
    "    neutral_tweets = (df_test[df_test.sentiment==2].text).tolist()\n",
    "    positive_tweets = (df_test[df_test.sentiment==4].text).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tweets=[]\n",
    "for string in negative_tweets:\n",
    "    words=nltk.word_tokenize(string.decode(\"utf-8\"))\n",
    "    negative_tweets.append((create_word_features(words),'positive'))\n",
    "    \n",
    "neutral_tweets=[]\n",
    "for string in neutral_tweets:\n",
    "    words=nltk.word_tokenize(string.decode(\"utf-8\"))\n",
    "    neutral_tweets.append((create_word_features(words),'neutral'))\n",
    "    \n",
    "positive_tweets=[]\n",
    "for string in neutral_tweets:\n",
    "    words=nltk.word_tokenize(string.decode(\"utf-8\"))\n",
    "    positive_tweets.append((create_word_features(words),'positive'))\n",
    "    \n",
    "test_set=positive_tweets+neutral_tweets+negative_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data = []\n",
    "reader = csv.reader(open(\"datatrain.csv\"),delimiter=',')\n",
    "stopWords = getStopWordList('stopwordsID.txt')\n",
    "\n",
    "for index,raw in enumerate(reader):\n",
    "    if index < 1001:\n",
    "        sentiment = raw[0]\n",
    "        tweet = raw[1]\n",
    "        tweet = processTweet(tweet)\n",
    "        tweet = removeTagsAndLinks(tweet)\n",
    "        tweet = getFeatureVector(tweet,stopWords)\n",
    "        tweet = ' '.join(tweet)\n",
    "        mydata = (tweet,sentiment)\n",
    "        list_data.append(mydata)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "json_dict = {}\n",
    "all_words = set(word.lower() for passage in list_data for word in word_tokenize(passage[0]))\n",
    "json_dict['corpus'] = [word for word in all_words] \n",
    "\n",
    "with open('corpus.json', 'w') as outfile:\n",
    "    json.dump(json_dict, outfile)\n",
    "\n",
    "t = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1]) for x in list_data]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "\n",
    "f = open('sentiment.pickle', 'wb')\n",
    "pickle.dump(classifier, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "f = open('sentiment.pickle', 'rb')\n",
    "classifier = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "list_tweet =  []\n",
    "list_kelas = []\n",
    "list_data = []\n",
    "\n",
    "data_test = csv.reader(open('datatest.csv', 'r'), delimiter=',')\n",
    "for data in data_test:\n",
    "    list_data.append(data)\n",
    "\n",
    "for data in list_data[:100]:\n",
    "    tweet = data[0]\n",
    "    tweet = processTweet(tweet)\n",
    "    tweet = removeTagsAndLinks(tweet)\n",
    "    tweet = getFeatureVector(tweet,stopWords)\n",
    "    tweet = ' '.join(tweet)\n",
    "    test_sent_features = {word.lower(): (word in word_tokenize(tweet.lower())) for word in all_words}\n",
    "    sentiment = classifier.classify(test_sent_features)\n",
    "    list_tweet.append(data[0])\n",
    "    list_kelas.append(sentiment)\n",
    "\n",
    "df = pd.DataFrame({'tweet': list_tweet,'kelas': list_kelas})\n",
    "df.to_csv('hasiltest.csv', sep=';', encoding='utf-8', index=False)\n",
    "test_set = csv.reader(open('hasiltest.csv', 'r'), delimiter=';')\n",
    "data_list = []\n",
    "\n",
    "for index,data in enumerate(test_set):\n",
    "    if index == 0:\n",
    "        pass\n",
    "    else:\n",
    "        sentiment = data[1]\n",
    "        tweet = data[0]\n",
    "        tweet = processTweet(tweet)\n",
    "        tweet = removeTagsAndLinks(tweet)\n",
    "        tweet = getFeatureVector(tweet,stopWords)\n",
    "        tweet = ' '.join(tweet)\n",
    "        mydata = (tweet,sentiment)\n",
    "        data_list.append(mydata)\n",
    "\n",
    "d = open('corpus.json')\n",
    "all_words = json.load(d)\n",
    "d.close\n",
    "\n",
    "all_words = all_words['corpus']\n",
    "\n",
    "test_set = [({word.lower() : (word.lower() in word_tokenize(x[0])) for word in all_words}, x[1]) for x in data_list[:100]]\n",
    "accuracy = nltk.classify.util.accuracy(classifier, test_set)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n"
     ]
    }
   ],
   "source": [
    "feature = classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
