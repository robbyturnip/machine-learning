{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi mengubah ke kata dasar\n",
    "def getstemmer():\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    \n",
    "    return stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi memanggil stopword\n",
    "def getstopword():\n",
    "    fp = open(r'stopwords.txt', 'r')\n",
    "    line = fp.readline()\n",
    "    stopwords = []\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopwords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    \n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlistdaerah():\n",
    "    files = ['provinsi.txt','kota.txt','kecamatan.txt','desa.txt']\n",
    "    daerah = []\n",
    "    \n",
    "    for file in files:\n",
    "        fp = open(file, 'r')\n",
    "        line = fp.readline()\n",
    "        \n",
    "        while line:\n",
    "            word = line.strip()\n",
    "            daerah.append(word)\n",
    "            line = fp.readline()\n",
    "        fp.close()\n",
    "    \n",
    "    return daerah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlistberita():\n",
    "    fp = open(r'list_berita.txt', 'r')\n",
    "    line = fp.readline()\n",
    "    berita = []\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        berita.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    \n",
    "    return berita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(tweet):\n",
    "    stopwords = getstopword()\n",
    "    stemmer   = getstemmer()\n",
    "    daerah    = getlistdaerah()\n",
    "    berita    = getlistberita()\n",
    "    # membuat string ke huruf kecil\n",
    "    tweet = tweet.lower()\n",
    "    # menghapus tab\n",
    "    tweet = tweet.strip()\n",
    "    # menghapus new line \n",
    "    tweet = tweet.replace('\\n',' ')\n",
    "    # menghapus at dan hastag\n",
    "    tweet = re.sub(\"\\w+@\\w+|@\\w+\",' ',tweet)\n",
    "    tweet = re.sub(\"\\w+#\\w+|#\\w+\",' ',tweet)\n",
    "    tweet = re.sub(\"wk\",\" \", tweet)\n",
    "    # menghapus character/link yang tidak digunakan dengan regex\n",
    "    tweet = re.sub(\"http([^\\s|,])+\",' ', tweet)\n",
    "    tweet = re.sub(\"www([^\\s|,])+\",' ', tweet)\n",
    "    tweet = re.sub(\"^rt[\\s]+\", ' ', tweet)\n",
    "    tweet = re.sub(\"[-()\\\"#%/@;:<>{}$`^+'”―=_~*&|.!?,]|\\d\",' ',tweet)\n",
    "    tweet = re.sub(\"(\\[|\\])\",\" \",tweet)\n",
    "    tweet = re.sub(\"\\d+\",' ', tweet)\n",
    "    # tokenize string\n",
    "    tweets = word_tokenize(tweet)\n",
    "    # menghapus beberapa kalimat yang tidak mengubah makna sentiment dalam kalimat dengan stopwords\n",
    "    tweet = []    \n",
    "    for word in tweets:\n",
    "        if word not in stopwords and word not in daerah and word not in berita:\n",
    "            word = word.encode('ascii', 'ignore').decode('ascii')\n",
    "            word = stemmer.stem(word)\n",
    "            tweet.append(word)\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    tweet = ' '.join(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_counter = CountVectorizer(ngram_range=(1, 2), analyzer='word', tokenizer=nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memanggil data set dan membagi menjadi data train dan data test\n",
    "df = pd.read_csv('data_train_label.csv',sep=',')\n",
    "x = df['text']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new = []\n",
    "for x in x_train:\n",
    "    x = preprocessing(x)\n",
    "    x_train_new.append(x)\n",
    "    \n",
    "df = pd.DataFrame({'text': x_train_new, 'sentiment': y_train})\n",
    "df.to_csv('cek_3.csv', sep=';', encoding='utf-8', index=False)\n",
    "x_train = ngram_counter.fit_transform(x_train_new)\n",
    "tf_idf = TfidfTransformer()\n",
    "x_train_new = tf_idf.fit_transform(x_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()\n",
    "model = classifier.fit(x_train_new.toarray(), y_train)\n",
    "f = open('sentiment_2.pickle', 'wb')\n",
    "pickle.dump(model, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hasil_prediksi = []\n",
    "list_tweet_test = []\n",
    "list_kelas_test = []\n",
    "y_test= [y for y in y_test]\n",
    "new_x_test = []\n",
    "tf_idf = TfidfTransformer()\n",
    "for index,tweet in enumerate(x_test):\n",
    "    sentiment = y_test[index].lower()\n",
    "    list_tweet_test.append(tweet)\n",
    "    list_kelas_test.append(sentiment)\n",
    "    tweet = preprocessing(tweet)\n",
    "    new_x_test.append(tweet)\n",
    "    tweet = [tweet]\n",
    "    tweet  = ngram_counter.transform(tweet)\n",
    "    tweet =  tf_idf.fit_transform(tweet.toarray())\n",
    "    prediksi  = model.predict(tweet.toarray())\n",
    "    list_hasil_prediksi.append(prediksi[0])\n",
    "\n",
    "df = pd.DataFrame({'text': list_tweet_test,'sentiment': list_kelas_test,'prediksi': list_hasil_prediksi})\n",
    "df.to_csv('hasil_2.csv', sep=';', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x_test  = ngram_counter.transform(new_x_test)\n",
    "new_x_test  = tf_idf.transform(new_x_test.toarray())\n",
    "result_prediction = model.predict(new_x_test.toarray())\n",
    "accuracy_score(result_prediction, y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ngram_counter.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
