{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk.classify\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fungsi mengubah ke kata dasar\n",
    "def getstemmer():\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    \n",
    "    return stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fungsi memanggil stopword\n",
    "def getstopword():\n",
    "    fp = open(r'stopwords.txt', 'r')\n",
    "    line = fp.readline()\n",
    "    stopwords = []\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopwords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    \n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fungsi untuk mengolah kata sebelum proses\n",
    "def preprocessing(tweet):\n",
    "    stopwords = getstopword()\n",
    "    stemmer   = getstemmer()\n",
    "    #membuat string ke huruf kecil\n",
    "    tweet = tweet.lower()\n",
    "    #menghapus character/link yang tidak digunakan dengan regex\n",
    "    tweet = re.sub(\"\\d+\",' ', tweet)\n",
    "    tweet = re.sub(\"http([^\\s|,])+\",' ', tweet)\n",
    "    tweet = re.sub(\"www([^\\s|,])+\",' ', tweet)\n",
    "    tweet = re.sub(\"^rt[\\s]+\", ' ', tweet)\n",
    "    tweet = re.sub(\"[-()\\\"#%/@;:<>{}$`^+'=~*&|.!?,]|\\d\",' ',tweet)\n",
    "    #tokenize string\n",
    "    tweets = word_tokenize(tweet)\n",
    "    #menghapus beberapa kalimat yang tidak mengubah makna sentiment dalam kalimat dengan stopwords\n",
    "    tweet = []    \n",
    "    for word in tweets:\n",
    "        if word not in stopwords: \n",
    "            word = stemmer.stem(word)\n",
    "            tweet.append(word)\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    tweet = ' '.join(tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#membuat kombinasi kata sesuai urutan\n",
    "def create_ngram_features(words, n=1):\n",
    "    words = nltk.word_tokenize(words)\n",
    "    ngram_vocab = ngrams(words, n)\n",
    "    my_dict = dict([(ng, True) for ng in ngram_vocab])\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "tweet_list = df['text'][:5000]\n",
    "sentiment_list = df['sentiment'][:5000]\n",
    "\n",
    "for index,tweet in enumerate(tweet_list):\n",
    "    tweet = preprocessing(tweet)\n",
    "    tweet_list[index] = tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membagi data train dan data test dari satu dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweet_list, sentiment_list, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.5\n"
     ]
    }
   ],
   "source": [
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "x_train = [x for x in x_train]\n",
    "x_test = [x for x in x_test]\n",
    "y_train = [x for x in y_train]\n",
    "y_test = [x for x in y_test]\n",
    "\n",
    "for index,tweet in enumerate (x_train): \n",
    "    sentiment = y_train[index]\n",
    "    tweet = create_ngram_features(tweet)\n",
    "    train_set.append((tweet,sentiment))\n",
    "    \n",
    "for index,tweet in enumerate (x_test): \n",
    "    sentiment = y_test[index]\n",
    "    tweet = create_ngram_features(tweet)\n",
    "    test_set.append((tweet,sentiment))\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "f = open('sentiment_1.pickle', 'wb')\n",
    "pickle.dump(classifier, f)\n",
    "f.close()\n",
    "\n",
    "accuracy = nltk.classify.util.accuracy(classifier, test_set)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "              ('lemah',) = True           Negati : Positi =     21.6 : 1.0\n",
      "            ('sejarah',) = True           Negati : Positi =     20.3 : 1.0\n",
      "             ('anjlok',) = True           Negati : Positi =     15.4 : 1.0\n",
      "              ('resmi',) = True           Positi : Negati =     13.8 : 1.0\n",
      "               ('buat',) = True           Positi : Negati =     13.8 : 1.0\n",
      "            ('selasar',) = True           Negati : Positi =     13.7 : 1.0\n",
      "           ('cikampek',) = True           Negati : Positi =     13.7 : 1.0\n",
      "               ('lika',) = True           Negati : Positi =     12.2 : 1.0\n",
      "               ('liku',) = True           Negati : Positi =     12.2 : 1.0\n",
      "             ('ambruk',) = True           Negati : Positi =     11.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
